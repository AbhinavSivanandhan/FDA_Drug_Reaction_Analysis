{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a589a0-3155-4886-8734-1ceed59f9866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1569, Total size: 480.86 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs:   4%|▎         | 18.5G/516G [03:30<1:31:13, 91.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing DataUnzip/226-drug-event-0003-of-0012.json/._drug-event-0003-of-0012.json: 'utf-8' codec can't decode byte 0xb0 in position 37: invalid start byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs:  30%|██▉       | 153G/516G [32:38<1:17:15, 78.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! 34.54% of files have been converted to Parquet.\n",
      "Missing files: 1027\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory containing the JSON files\n",
    "DATA_DIR = \"DataUnzip\"\n",
    "OUTPUT_DIR = \"parquet_database\"\n",
    "\n",
    "# Progress tracking file\n",
    "PROGRESS_FILE = \"progress_log.json\"\n",
    "ERROR_LOG_FILE = \"error_log.json\"\n",
    "\n",
    "# Partition column (e.g., receivedate if available)\n",
    "PARTITION_COLUMN = \"receivedate\"\n",
    "MAX_PARTITIONS = 1024  # Threshold for maximum allowable partitions\n",
    "\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load progress from log file.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"processed_files\": [], \"processed_size\": 0}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save progress to log file.\"\"\"\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "def log_error(file_path, error_message):\n",
    "    \"\"\"Log errors to the error log file.\"\"\"\n",
    "    error_log = []\n",
    "    if os.path.exists(ERROR_LOG_FILE):\n",
    "        with open(ERROR_LOG_FILE, \"r\") as f:\n",
    "            error_log = json.load(f)\n",
    "    error_log.append({\"file\": file_path, \"error\": error_message})\n",
    "    with open(ERROR_LOG_FILE, \"w\") as f:\n",
    "        json.dump(error_log, f, indent=4)\n",
    "\n",
    "def get_total_size_and_files(data_dir):\n",
    "    \"\"\"Get total size and list of all JSON files.\"\"\"\n",
    "    total_size = 0\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                total_size += os.path.getsize(full_path)\n",
    "                all_files.append(full_path)\n",
    "    return total_size, all_files\n",
    "\n",
    "def process_file_to_dataframe(file_path):\n",
    "    \"\"\"Process a single JSON file into a Pandas DataFrame.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "        # Flatten JSON into a Pandas DataFrame\n",
    "        if isinstance(data, list):\n",
    "            df = pd.json_normalize(data)\n",
    "        elif isinstance(data, dict) and \"results\" in data:\n",
    "            df = pd.json_normalize(data[\"results\"])\n",
    "        else:\n",
    "            df = pd.json_normalize([data])  # Single record\n",
    "        \n",
    "        # Convert receivedate to Year-Month for lower cardinality\n",
    "        if PARTITION_COLUMN in df.columns:\n",
    "            df[PARTITION_COLUMN] = pd.to_datetime(\n",
    "                df[PARTITION_COLUMN], format='%Y%m%d', errors='coerce'\n",
    "            ).dt.to_period('M').astype(str)  # Convert to YYYY-MM format\n",
    "        return df\n",
    "\n",
    "def save_to_parquet(df, output_dir, partition_column=None):\n",
    "    \"\"\"Save DataFrame to Parquet with optional partitioning.\"\"\"\n",
    "    # Determine the number of unique partitions\n",
    "    num_partitions = df[partition_column].nunique() if partition_column in df.columns else 0\n",
    "\n",
    "    # Skip partitioning if too many partitions\n",
    "    if partition_column and num_partitions > MAX_PARTITIONS:\n",
    "        print(f\"Warning: Skipping partitioning for file due to {num_partitions} unique partitions.\")\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, os.path.join(output_dir, \"non_partitioned.parquet\"), existing_data_behavior=\"overwrite_or_ignore\")\n",
    "    else:\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_to_dataset(\n",
    "            table,\n",
    "            root_path=output_dir,\n",
    "            partition_cols=[partition_column] if partition_column else None,\n",
    "            existing_data_behavior=\"overwrite_or_ignore\"\n",
    "        )\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Load progress\n",
    "    progress = load_progress()\n",
    "\n",
    "    # Get total size and list of files\n",
    "    total_size, all_files = get_total_size_and_files(DATA_DIR)\n",
    "    processed_size = progress[\"processed_size\"]\n",
    "    processed_files = set(progress[\"processed_files\"])\n",
    "\n",
    "    print(f\"Total files: {len(all_files)}, Total size: {total_size / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_size, initial=processed_size, unit=\"B\", unit_scale=True, desc=\"Processing JSONs\") as pbar:\n",
    "        for file_path in all_files:\n",
    "            file_name = Path(file_path).name\n",
    "\n",
    "            if file_name in processed_files:\n",
    "                continue  # Skip already processed files\n",
    "\n",
    "            try:\n",
    "                # Convert JSON to Pandas DataFrame\n",
    "                df = process_file_to_dataframe(file_path)\n",
    "\n",
    "                # Save to partitioned Parquet\n",
    "                save_to_parquet(df, OUTPUT_DIR, partition_column=PARTITION_COLUMN)\n",
    "\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                processed_size += file_size\n",
    "                processed_files.add(file_name)\n",
    "\n",
    "                # Update progress\n",
    "                progress[\"processed_size\"] = processed_size\n",
    "                progress[\"processed_files\"] = list(processed_files)\n",
    "                save_progress(progress)\n",
    "\n",
    "                pbar.update(file_size)\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                log_error(file_path, error_message)\n",
    "                print(f\"Error processing {file_path}: {error_message}\")\n",
    "\n",
    "    # Verify completion\n",
    "    completion_percentage = len(processed_files) / len(all_files) * 100\n",
    "    print(f\"Processing complete! {completion_percentage:.2f}% of files have been converted to Parquet.\")\n",
    "    if len(processed_files) < len(all_files):\n",
    "        print(f\"Missing files: {len(all_files) - len(processed_files)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482e364-0292-4785-8514-902506a8d609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
