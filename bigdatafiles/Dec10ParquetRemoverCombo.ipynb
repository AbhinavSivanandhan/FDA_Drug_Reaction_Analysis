{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ee1c78-a55d-41b9-81ee-a9f959f9a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 0, Total size: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs: 20.0GB [00:00, ?B/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 167\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_files)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(processed_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[7], line 160\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Verify completion\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m completion_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(processed_files) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_files) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete! \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletion_percentage\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of files have been converted to Parquet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed_files) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_files):\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory containing the JSON files\n",
    "DATA_DIR = \"data\"\n",
    "OUTPUT_DIR = \"parquet_database\"\n",
    "\n",
    "# Progress and error tracking files\n",
    "PROGRESS_FILE = \"progress_log.json\"\n",
    "ERROR_LOG_FILE = \"error_log.json\"\n",
    "\n",
    "# Partition column (e.g., receivedate if available)\n",
    "PARTITION_COLUMN = \"receivedate\"\n",
    "MAX_PARTITIONS = 1024  # Threshold for maximum allowable partitions\n",
    "\n",
    "# Fields to remove\n",
    "FIELDS_TO_REMOVE = [\n",
    "    \"rxcui\",\n",
    "    \"product_ndc\",\n",
    "    \"spl_id\",\n",
    "    \"spl_set_id\",\n",
    "    \"package_ndc\",\n",
    "    \"nui\",\n",
    "    \"application_number\"\n",
    "]\n",
    "\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load progress from log file.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"processed_files\": [], \"processed_size\": 0}\n",
    "\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save progress to log file.\"\"\"\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(progress, f)\n",
    "\n",
    "\n",
    "def log_error(file_path, error_message):\n",
    "    \"\"\"Log errors to the error log file.\"\"\"\n",
    "    error_log = []\n",
    "    if os.path.exists(ERROR_LOG_FILE):\n",
    "        with open(ERROR_LOG_FILE, \"r\") as f:\n",
    "            error_log = json.load(f)\n",
    "    error_log.append({\"file\": file_path, \"error\": error_message})\n",
    "    with open(ERROR_LOG_FILE, \"w\") as f:\n",
    "        json.dump(error_log, f, indent=4)\n",
    "\n",
    "\n",
    "def get_total_size_and_files(data_dir):\n",
    "    \"\"\"Get total size and list of all JSON files.\"\"\"\n",
    "    total_size = 0\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                total_size += os.path.getsize(full_path)\n",
    "                all_files.append(full_path)\n",
    "    return total_size, all_files\n",
    "\n",
    "\n",
    "def process_file_to_dataframe(file_path):\n",
    "    \"\"\"Process a single JSON file into a Pandas DataFrame.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Flatten JSON into a Pandas DataFrame\n",
    "        if isinstance(data, list):\n",
    "            df = pd.json_normalize(data)\n",
    "        elif isinstance(data, dict) and \"results\" in data:\n",
    "            df = pd.json_normalize(data[\"results\"])\n",
    "        else:\n",
    "            df = pd.json_normalize([data])  # Single record\n",
    "\n",
    "        # Remove unnecessary fields\n",
    "        df.drop(columns=[col for col in FIELDS_TO_REMOVE if col in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "        # Convert receivedate to Year-Month for lower cardinality\n",
    "        if PARTITION_COLUMN in df.columns:\n",
    "            df[PARTITION_COLUMN] = pd.to_datetime(\n",
    "                df[PARTITION_COLUMN], format='%Y%m%d', errors='coerce'\n",
    "            ).dt.to_period('M').astype(str)  # Convert to YYYY-MM format\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def save_to_parquet(df, output_dir, partition_column=None):\n",
    "    \"\"\"Save DataFrame to Parquet with optional partitioning.\"\"\"\n",
    "    # Determine the number of unique partitions\n",
    "    num_partitions = df[partition_column].nunique() if partition_column in df.columns else 0\n",
    "\n",
    "    # Skip partitioning if too many partitions\n",
    "    if partition_column and num_partitions > MAX_PARTITIONS:\n",
    "        print(f\"Warning: Skipping partitioning for file due to {num_partitions} unique partitions.\")\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, os.path.join(output_dir, \"non_partitioned.parquet\"), existing_data_behavior=\"overwrite_or_ignore\")\n",
    "    else:\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_to_dataset(\n",
    "            table,\n",
    "            root_path=output_dir,\n",
    "            partition_cols=[partition_column] if partition_column else None,\n",
    "            existing_data_behavior=\"overwrite_or_ignore\"\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Load progress\n",
    "    progress = load_progress()\n",
    "\n",
    "    # Get total size and list of files\n",
    "    total_size, all_files = get_total_size_and_files(DATA_DIR)\n",
    "    processed_size = progress[\"processed_size\"]\n",
    "    processed_files = set(progress[\"processed_files\"])\n",
    "\n",
    "    print(f\"Total files: {len(all_files)}, Total size: {total_size / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_size, initial=processed_size, unit=\"B\", unit_scale=True, desc=\"Processing JSONs\") as pbar:\n",
    "        for file_path in all_files:\n",
    "            file_name = Path(file_path).name\n",
    "\n",
    "            if file_name in processed_files:\n",
    "                continue  # Skip already processed files\n",
    "\n",
    "            try:\n",
    "                # Convert JSON to Pandas DataFrame\n",
    "                df = process_file_to_dataframe(file_path)\n",
    "\n",
    "                # Save to partitioned Parquet\n",
    "                save_to_parquet(df, OUTPUT_DIR, partition_column=PARTITION_COLUMN)\n",
    "\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                processed_size += file_size\n",
    "                processed_files.add(file_name)\n",
    "\n",
    "                # Update progress\n",
    "                progress[\"processed_size\"] = processed_size\n",
    "                progress[\"processed_files\"] = list(processed_files)\n",
    "                save_progress(progress)\n",
    "\n",
    "                pbar.update(file_size)\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                log_error(file_path, error_message)\n",
    "                print(f\"Error processing {file_path}: {error_message}\")\n",
    "\n",
    "    # Verify completion\n",
    "    completion_percentage = len(processed_files) / len(all_files) * 100\n",
    "    print(f\"Processing complete! {completion_percentage:.2f}% of files have been converted to Parquet.\")\n",
    "    if len(processed_files) < len(all_files):\n",
    "        print(f\"Missing files: {len(all_files) - len(processed_files)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaddf3-7f68-4040-b979-2fad2950d2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
